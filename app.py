# app.py
import os
import re
import json
import time
import shutil
import tempfile
from pathlib import Path
from typing import List, Dict, Tuple, Optional, Any
from difflib import SequenceMatcher

import streamlit as st
from dotenv import load_dotenv
from pydub import AudioSegment
from pydub.silence import detect_silence
import azure.cognitiveservices.speech as speechsdk

try:
    import pandas as pd
except Exception:
    pd = None


# =========================
# INIT
# =========================
load_dotenv()
st.set_page_config(page_title="Speaking Checker ‚Äî Level 1", layout="wide")
st.title("Speaking Checker ‚Äî Level 1")
st.caption("Tab 1: Ch·∫•m theo vocab/phrase (Table A) | Tab 2: Ch·∫•m ƒë·ªçc ƒëo·∫°n vƒÉn (Reading Passage)")


# =========================
# BASIC CHECKS
# =========================
def has_ffmpeg() -> bool:
    return shutil.which("ffmpeg") is not None


# =========================
# TOKEN HELPERS
# =========================
def _norm_token(s: str) -> str:
    s = (s or "").strip().lower()
    out = []
    for ch in s:
        if ch.isalnum() or ch == "'":
            out.append(ch)
    return "".join(out)


def _item_tokens(item: str) -> List[str]:
    return [_norm_token(x) for x in (item or "").split() if _norm_token(x)]


def _split_to_tokens(text: str) -> List[str]:
    raw = (text or "").replace("\n", " ").strip()
    toks = []
    for p in raw.split():
        t = _norm_token(p)
        if t:
            toks.append(t)
    return toks


def guess_student_name(filename: str) -> str:
    base = Path(filename).stem
    base = base.replace("_", " ").strip()
    return base


# =========================
# AUDIO HELPERS
# =========================
def to_wav_16k_mono(src_path: str, out_path: str) -> Tuple[int, int]:
    """
    Convert audio -> 16kHz mono WAV (PCM 16-bit).
    Requires ffmpeg for mp3/m4a/webm/ogg.
    Returns (duration_ms, 16000)
    """
    audio = AudioSegment.from_file(src_path)
    duration_ms = len(audio)
    audio = audio.set_channels(1).set_frame_rate(16000).set_sample_width(2)
    h = audio.export(out_path, format="wav")
    try:
        h.close()
    except Exception:
        pass
    return duration_ms, 16000


def analyze_audio_quality(
    wav_path: str,
    min_duration_s: float = 5.0,
    min_dbfs: float = -35.0,
    max_silence_ratio: float = 0.65,
    min_silence_len_ms: int = 500,
    silence_rel_db: float = 16.0,
    long_pause_ms: int = 1000,
) -> Tuple[Dict[str, float], List[str]]:
    """
    QC gate:
      - too short
      - too quiet (dBFS too low)
      - too much silence ratio
    """
    audio = AudioSegment.from_file(wav_path)
    dur_ms = max(1, len(audio))
    duration_s = dur_ms / 1000.0

    dbfs = audio.dBFS
    if dbfs == float("-inf"):
        dbfs = -100.0

    rms = float(audio.rms or 0)

    silence_thresh = max(dbfs - float(silence_rel_db), -60.0)
    sil = detect_silence(
        audio,
        min_silence_len=int(min_silence_len_ms),
        silence_thresh=float(silence_thresh),
    )

    silence_ms = 0
    long_pause_count = 0
    for s, e in sil:
        seg = max(0, int(e) - int(s))
        silence_ms += seg
        if seg >= int(long_pause_ms):
            long_pause_count += 1

    silence_ratio = silence_ms / float(dur_ms)
    silence_total_s = silence_ms / 1000.0

    issues = []
    if duration_s < float(min_duration_s):
        issues.append(f"Audio qu√° ng·∫Øn ({duration_s:.1f}s < {min_duration_s:.1f}s)")
    if dbfs < float(min_dbfs):
        issues.append(f"N√≥i nh·ªè/volume th·∫•p ({dbfs:.1f} dBFS < {min_dbfs:.1f} dBFS)")
    if silence_ratio > float(max_silence_ratio):
        issues.append(f"Ng·∫Øt ngh·ªâ qu√° nhi·ªÅu (silence {silence_ratio:.0%} > {max_silence_ratio:.0%})")

    metrics = {
        "duration_s": float(duration_s),
        "dbfs": float(dbfs),
        "rms": float(rms),
        "silence_ratio": float(silence_ratio),
        "silence_total_s": float(silence_total_s),
        "silence_thresh_dbfs": float(silence_thresh),
        "long_pause_count": float(long_pause_count),
    }
    return metrics, issues


# =========================
# AZURE HELPERS
# =========================
def _safe_get_jsonresult_from_properties(props, prop_id) -> Optional[str]:
    raw_json = None
    if hasattr(props, "get_property"):
        try:
            raw_json = props.get_property(prop_id)
        except Exception:
            raw_json = None
    if not raw_json and hasattr(props, "get"):
        try:
            raw_json = props.get(prop_id)
        except Exception:
            raw_json = None
    if not raw_json and hasattr(props, "items"):
        try:
            for k, v in props.items():
                if "jsonresult" in str(k).lower():
                    raw_json = v
                    break
        except Exception:
            pass
    return raw_json


def _set_timeout_safe(
    speech_config: speechsdk.SpeechConfig,
    prop_id_attr: str,
    prop_name_fallback: str,
    value_ms: int,
) -> None:
    v = str(int(value_ms))
    pid = getattr(speechsdk.PropertyId, prop_id_attr, None)
    if pid is not None:
        try:
            speech_config.set_property(pid, v)
            return
        except Exception:
            pass
    if hasattr(speech_config, "set_property_by_name"):
        try:
            speech_config.set_property_by_name(prop_name_fallback, v)
            return
        except Exception:
            pass


def _enable_prosody_best_effort(pa_config, enable: bool) -> None:
    if not enable:
        return
    try:
        pa_config.enable_prosody_assessment()  # Method chu·∫©n theo docs
        return
    except Exception:
        # N·∫øu version SDK c≈© qu√° (<1.35), s·∫Ω raise ‚Üí c√≥ th·ªÉ log warning
        pass


def _is_end_of_stream_cancel(reason_str: str, details_str: str) -> bool:
    rs = (reason_str or "").lower()
    ds = (details_str or "").lower()
    return ("endofstream" in rs) or ("endofstream" in ds)


def run_pron_assessment_continuous(
    wav_path: str,
    reference_text: str,
    locale: str,
    speech_key: str,
    speech_region: str,
    end_silence_timeout_ms: int,
    seg_silence_timeout_ms: int,
    initial_silence_timeout_ms: int,
    max_wait_seconds: int = 120,
    enable_prosody: bool = True,
) -> List[dict]:
    """
    Continuous recognition over file (Pronunciation Assessment).
    """
    speech_config = speechsdk.SpeechConfig(subscription=speech_key, region=speech_region)
    speech_config.speech_recognition_language = locale

    _set_timeout_safe(
        speech_config,
        "SpeechServiceConnection_EndSilenceTimeoutMs",
        "SpeechServiceConnection_EndSilenceTimeoutMs",
        end_silence_timeout_ms,
    )
    _set_timeout_safe(
        speech_config,
        "SpeechServiceConnection_InitialSilenceTimeoutMs",
        "SpeechServiceConnection_InitialSilenceTimeoutMs",
        initial_silence_timeout_ms,
    )
    _set_timeout_safe(
        speech_config,
        "SpeechServiceConnection_SegmentationSilenceTimeoutMs",
        "SpeechServiceConnection_SegmentationSilenceTimeoutMs",
        seg_silence_timeout_ms,
    )

    audio_config = speechsdk.audio.AudioConfig(filename=wav_path)
    recognizer = speechsdk.SpeechRecognizer(speech_config=speech_config, audio_config=audio_config)

    pa_config = speechsdk.PronunciationAssessmentConfig(
        reference_text=reference_text,
        grading_system=speechsdk.PronunciationAssessmentGradingSystem.HundredMark,
        granularity=speechsdk.PronunciationAssessmentGranularity.Phoneme,
        enable_miscue=False,  # Kh√¥ng h·ªó tr·ª£ trong continuous mode ‚Üí t·∫Øt lu√¥n
    )

    # Ch·ªâ b·∫≠t prosody n·∫øu locale l√† en-US v√† user b·∫≠t
    if enable_prosody and locale == "en-US":
        try:
            pa_config.enable_prosody_assessment()
        except Exception:
            pass  # SDK c≈© s·∫Ω b·ªè qua

    pa_config.apply_to(recognizer)

    results_json: List[dict] = []
    done = {"flag": False}
    cancel_info = {"reason": None, "details": None, "is_fatal": False}

    def on_recognized(evt):
        r = evt.result
        if r.reason == speechsdk.ResultReason.RecognizedSpeech:
            raw = _safe_get_jsonresult_from_properties(
                r.properties, speechsdk.PropertyId.SpeechServiceResponse_JsonResult
            )
            if raw:
                try:
                    results_json.append(json.loads(raw))
                except Exception:
                    pass

    def on_canceled(evt):
        reason_str, details_str = "", ""
        try:
            details = speechsdk.CancellationDetails.from_result(evt.result)
            reason_str = str(details.reason)
            details_str = str(details.error_details or "")
        except Exception:
            try:
                cd = evt.result.cancellation_details
                reason_str = str(cd.reason)
                details_str = str(cd.error_details or "")
            except Exception:
                reason_str, details_str = "Canceled", ""

        cancel_info["reason"] = reason_str
        cancel_info["details"] = details_str

        if _is_end_of_stream_cancel(reason_str, details_str):
            cancel_info["is_fatal"] = False
            done["flag"] = True
            return

        cancel_info["is_fatal"] = True
        done["flag"] = True

    def on_session_stopped(evt):
        done["flag"] = True

    recognizer.recognized.connect(on_recognized)
    recognizer.canceled.connect(on_canceled)
    recognizer.session_stopped.connect(on_session_stopped)

    started = False
    try:
        recognizer.start_continuous_recognition()
        started = True
        start_t = time.time()
        while not done["flag"]:
            time.sleep(0.1)
            if (time.time() - start_t) > max_wait_seconds:
                cancel_info["reason"] = "Timeout"
                cancel_info["details"] = "max_wait_seconds exceeded"
                cancel_info["is_fatal"] = False
                done["flag"] = True
    finally:
        if started:
            try:
                recognizer.stop_continuous_recognition()
            except Exception:
                pass

    if cancel_info["is_fatal"] and not results_json:
        raise RuntimeError(f"Azure canceled: {cancel_info['reason']} | {cancel_info['details']}")

    return results_json

def call_with_retry(fn, max_retries: int = 3, base_sleep: float = 0.9):
    last_err = None
    for i in range(max_retries):
        try:
            return fn()
        except Exception as e:
            msg = str(e).lower()
            last_err = e
            if "endofstream" in msg:
                return []
            if (
                ("1007" in msg)
                or ("connection was closed" in msg)
                or ("remote host" in msg)
                or ("validate speech context" in msg)
                or ("wsarecv" in msg)
            ):
                time.sleep(base_sleep * (1.6 ** i))
                continue
            raise
    raise last_err


# =========================
# JSON PARSE HELPERS
# =========================
def _tick_to_ms(v) -> Optional[float]:
    try:
        if v is None:
            return None
        vv = float(v)
        if vv <= 0:
            return 0.0
        return vv / 10000.0
    except Exception:
        return None


def extract_word_rows(result_json: dict) -> List[Dict]:
    """
    Flatten Azure JSON to rows per recognized word.
    Includes phonemes_detail and word timestamps (best-effort).
    """
    rows: List[Dict] = []
    nbest = result_json.get("NBest", [])
    if not nbest:
        return rows

    prosody = None
    try:
        pa0 = (nbest[0].get("PronunciationAssessment", {}) or {})
        prosody = pa0.get("ProsodyScore", None)
    except Exception:
        prosody = None

    words = nbest[0].get("Words", []) or []
    for w in words:
        word = w.get("Word", "") or ""
        pa = w.get("PronunciationAssessment", {}) or {}
        accuracy = pa.get("AccuracyScore", None)
        error_type = (pa.get("ErrorType", "None") or "None").strip()

        offset_ms = _tick_to_ms(w.get("Offset"))
        dur_ms = _tick_to_ms(w.get("Duration"))
        end_ms = None
        if isinstance(offset_ms, (int, float)) and isinstance(dur_ms, (int, float)):
            end_ms = float(offset_ms) + float(dur_ms)

        phonemes_detail = []
        for ph in (w.get("Phonemes", []) or []):
            ph_pa = (ph.get("PronunciationAssessment", {}) or {})
            ph_acc = ph_pa.get("AccuracyScore", None)
            ph_txt = ph.get("Phoneme") or ph.get("PhonemeText") or ""
            phonemes_detail.append({"ph": ph_txt, "acc": ph_acc})

        rows.append(
            {
                "word": word,
                "tok": _norm_token(word),
                "accuracy": accuracy,
                "error_type": error_type,
                "phonemes_detail": phonemes_detail,
                "prosody": prosody,
                "offset_ms": offset_ms,
                "dur_ms": dur_ms,
                "end_ms": end_ms,
            }
        )
    return rows


# =========================
# ALIGNMENT (monotonic)
# =========================
def _token_match(expected: str, heard: str) -> bool:
    if expected == heard:
        return True
    # plural tolerance
    if expected.endswith("s") and expected[:-1] == heard:
        return True
    if heard.endswith("s") and heard[:-1] == expected:
        return True
    variants = {"realisation": "realization", "realization": "realisation"}
    if variants.get(expected) == heard:
        return True
    return False


def map_expected_to_recognized(expected_tokens: List[str], rec_tokens: List[str], lookahead: int = 4) -> List[Optional[int]]:
    j = 0
    mapping: List[Optional[int]] = []
    n = len(rec_tokens)
    for et in expected_tokens:
        found = None
        for jj in range(j, min(n, j + lookahead + 1)):
            if _token_match(et, rec_tokens[jj]):
                found = jj
                break
        mapping.append(found)
        if found is not None:
            j = found + 1
    return mapping


# =========================
# DETECTORS
# =========================
BAD_TYPES = {"Mispronunciation", "Omission", "Insertion"}
S_PHONEMES = {"s", "z", "S", "Z"}
# Phoneme sets (best-effort). We only flag "missing final sound" if tail phoneme is consonant.
VOWELISH_PHONEMES = {
    "aa","ae","ah","ao","aw","ax","ay",
    "eh","el","em","en","er","ey",
    "ih","iy",
    "ow","oy",
    "uh","uw",
    "y","w",
}

CONSONANTISH_PHONEMES = {
    "b","d","f","g","hh","jh","k","l","m","n","ng","p","r","s","t","th","v","w","y","z",
    "ch","sh","zh","dh",
    "c","q","x",
}

def detect_missing_plural_s(expected_tok: str, heard_tok: str) -> bool:
    return expected_tok.endswith("s") and expected_tok[:-1] == heard_tok


def detect_missing_s_sound(row: Dict, s_acc_threshold: float) -> Tuple[bool, bool, Optional[str]]:
    """
    Returns (has_issue, is_final, phoneme_text)
    """
    phs = row.get("phonemes_detail") or []
    if not phs:
        return False, False, None

    exp_seq = [(p.get("ph") or "").strip() for p in phs]
    acc_seq = [p.get("acc", None) for p in phs]

    if exp_seq:
        last_ph = exp_seq[-1]
        last_acc = acc_seq[-1]
        if last_ph in S_PHONEMES and isinstance(last_acc, (int, float)) and last_acc < s_acc_threshold:
            return True, True, last_ph

    for ph, acc in zip(exp_seq, acc_seq):
        if ph in S_PHONEMES and isinstance(acc, (int, float)) and acc < s_acc_threshold:
            return True, False, ph

    return False, False, None


def detect_missing_final_sound(row: Dict, final_acc_threshold: float) -> Optional[str]:
    """
    Flag 'missing final sound' ONLY if the weak tail phoneme is consonant-ish.
    Avoid vowel tails like /iy/ /ay/ /ow/ etc.
    Return tail phoneme label (best effort) or None.
    """
    phs = row.get("phonemes_detail") or []
    if not phs:
        return None

    tail = phs[-2:] if len(phs) >= 2 else phs[-1:]

    for p in tail[::-1]:
        ph = (p.get("ph") or "").strip()
        acc = p.get("acc", None)
        if not ph or not isinstance(acc, (int, float)):
            continue

        ph_norm = ph.lower()
        ph_norm = re.sub(r"[^a-z]", "", ph_norm)  # ay1 -> ay, iy0 -> iy, sh2 -> sh

        # b·ªè qua vowel tails
        if ph_norm in VOWELISH_PHONEMES:
            continue

        is_consonantish = (
            ph_norm in CONSONANTISH_PHONEMES
            or ph_norm in {"sh", "ch", "th", "ng", "jh", "zh", "dh"}
            or (len(ph_norm) <= 3 and ph_norm.isalpha() and ph_norm not in VOWELISH_PHONEMES)
        )

        if is_consonantish and float(acc) < float(final_acc_threshold):
            return ph

    return None

# =========================
# SCORING (word-based)
# =========================
def compute_word_based_score(
    expected_tokens: List[str],
    rec_rows: List[Dict],
    lookahead: int,
    missing_token_score: float = 0.0,
) -> Dict[str, object]:
    rec_tokens = [r.get("tok") or _norm_token(r.get("word", "")) for r in rec_rows]
    mapping = map_expected_to_recognized(expected_tokens, rec_tokens, lookahead=lookahead)

    total = len(expected_tokens)
    if total == 0:
        return {"score_pct": None, "missing": 0, "total": 0, "avg_found": None, "mapping": mapping}

    scores = []
    found_scores = []
    missing = 0

    for et, mi in zip(expected_tokens, mapping):
        if mi is None:
            scores.append(float(missing_token_score))
            missing += 1
        else:
            acc = rec_rows[mi].get("accuracy", None)
            s = float(acc) if isinstance(acc, (int, float)) else 0.0
            scores.append(s)
            found_scores.append(s)

    score_pct = sum(scores) / float(total)
    avg_found = (sum(found_scores) / len(found_scores)) if found_scores else None
    return {
        "score_pct": float(score_pct),
        "missing": int(missing),
        "total": int(total),
        "avg_found": float(avg_found) if avg_found is not None else None,
        "mapping": mapping,
    }


# =========================
# TAB 1 HELPERS (Table A)
# =========================
def fmt_entry(phrase: str, word: str, extra: str = "") -> str:
    extra = (extra or "").strip()
    if extra and not extra.startswith("("):
        extra = f"({extra})"
    if phrase.strip().lower() == word.strip().lower():
        return f"{word}{extra}"
    return f"{phrase}: {word}{extra}"


def uniq_keep_order(items: List[str], limit: int = 80) -> List[str]:
    seen = set()
    out = []
    for x in items:
        x = (x or "").strip()
        if not x:
            continue
        if x not in seen:
            out.append(x)
            seen.add(x)
        if len(out) >= limit:
            break
    return out


SEVERITY = {
    "omission": 6,
    "mispron": 5,
    "missing_final": 4,
    "missing_s": 3,
    "missing_plural_s": 2,
    "low_acc": 1,
    "prosody": 0,
}


def build_student_error_buckets_and_issues(
    vocab_list: List[str],
    all_rows: List[Dict],
    lookahead: int,
    accuracy_threshold: float,
    s_acc_threshold: float,
    final_acc_threshold: float,
    enable_prosody: bool,
    prosody_threshold: float,
) -> Tuple[Dict[str, List[str]], List[Dict]]:
    rec_rows = [r for r in all_rows if (r.get("word") or "").strip()]
    rec_tokens = [_norm_token(r["word"]) for r in rec_rows]

    expected_tokens: List[str] = []
    spans: List[Tuple[int, int]] = []
    cursor = 0
    for item in vocab_list:
        toks = _item_tokens(item)
        start = cursor
        expected_tokens.extend(toks)
        cursor += len(toks)
        spans.append((start, cursor))

    mapping = map_expected_to_recognized(expected_tokens, rec_tokens, lookahead=lookahead)

    buckets = {
        "mispron_low": [],
        "missing_final": [],
        "missing_s": [],
        "missing_plural_s": [],
        "prosody": [],
        "omission": [],
    }
    issues_struct: List[Dict] = []

    def add_issue(cat: str, phrase: str, heard_word: str, extra: str, label: str):
        issues_struct.append(
            {
                "cat": cat,
                "severity": SEVERITY.get(cat, 0),
                "phrase": phrase,
                "word": heard_word,
                "extra": extra,
                "label": label,
                "entry": fmt_entry(phrase, heard_word, extra),
            }
        )

    for item_idx, item in enumerate(vocab_list):
        start, end = spans[item_idx]
        toks = expected_tokens[start:end]
        mapped = mapping[start:end]

        if not toks:
            continue

        if any(x is None for x in mapped):
            buckets["omission"].append(item)
            add_issue("omission", item, item, "", "(thi·∫øu t·ª´)")
            continue

        for expected_tok, rec_i in zip(toks, mapped):
            row = rec_rows[rec_i]
            heard_word = (row.get("word") or expected_tok).strip()
            heard_tok = _norm_token(heard_word)

            et = (row.get("error_type") or "None").strip()
            acc = row.get("accuracy", None)

            if et in BAD_TYPES:
                buckets["mispron_low"].append(fmt_entry(item, heard_word, ""))
                add_issue("mispron", item, heard_word, "", "(ph√°t √¢m sai)")
            elif isinstance(acc, (int, float)) and acc < accuracy_threshold:
                buckets["mispron_low"].append(fmt_entry(item, heard_word, ""))
                add_issue("low_acc", item, heard_word, "", "(ch∆∞a chu·∫©n)")

            if detect_missing_plural_s(expected_tok, heard_tok):
                buckets["missing_plural_s"].append(fmt_entry(item, heard_word, ""))
                add_issue("missing_plural_s", item, heard_word, "", "(thi·∫øu -s s·ªë nhi·ªÅu)")

            has_s, _, s_ph = detect_missing_s_sound(row, s_acc_threshold=s_acc_threshold)
            if has_s:
                ph_txt = (s_ph or "").lower()
                extra = f"/{ph_txt}/" if ph_txt else ""
                buckets["missing_s"].append(fmt_entry(item, heard_word, extra))
                if ph_txt == "s":
                    add_issue("missing_s", item, heard_word, "/s/", "(thi·∫øu /s/)")
                elif ph_txt == "z":
                    add_issue("missing_s", item, heard_word, "/z/", "(thi·∫øu /z/)")
                else:
                    add_issue("missing_s", item, heard_word, extra, "(thi·∫øu √¢m)")

            final_ph = detect_missing_final_sound(row, final_acc_threshold=final_acc_threshold)
            if final_ph:
                extra = f"/{final_ph}/"
                buckets["missing_final"].append(fmt_entry(item, heard_word, extra))
                add_issue("missing_final", item, heard_word, extra,"")

            if enable_prosody:
                p = row.get("prosody", None)
                if isinstance(p, (int, float)) and p < prosody_threshold:
                    buckets["prosody"].append(fmt_entry(item, heard_word, ""))
                    add_issue("prosody", item, heard_word, "", "(prosody)")

    for k in list(buckets.keys()):
        buckets[k] = uniq_keep_order(buckets[k], limit=80)

    return buckets, issues_struct


def summarize_worst_per_word(issues_struct: List[Dict], limit: int = 60) -> List[str]:
    best: Dict[str, Dict] = {}
    for it in issues_struct:
        phrase = (it.get("phrase") or "").strip()
        word = (it.get("word") or "").strip()
        key = f"{phrase}|||{word}"
        if key not in best:
            best[key] = it
        else:
            if int(it.get("severity", 0)) > int(best[key].get("severity", 0)):
                best[key] = it

    items = list(best.values())
    items.sort(key=lambda x: (-int(x.get("severity", 0)), x.get("entry", "")))

    out = []
    for it in items:
        entry = (it.get("entry") or "").strip()
        label = (it.get("label") or "").strip()
        if entry and label:
            out.append(f"{entry} {label}".strip())
        elif entry:
            out.append(entry)
        if len(out) >= int(limit):
            break

    return out


# =========================
# TAB 2 HELPERS (Passage)
# =========================
def split_passage_sentences(passage_text: str) -> List[str]:
    txt = (passage_text or "").strip()
    if not txt:
        return []
    txt = re.sub(r"\s+", " ", txt)
    sents = re.split(r"(?<=[.!?])\s+", txt)
    out = []
    for s in sents:
        s = (s or "").strip()
        if s:
            out.append(s)
    return out


def build_sentence_spans(passage_text: str) -> Tuple[List[str], List[Dict]]:
    sents = split_passage_sentences(passage_text)
    spans = []
    expected_tokens = []
    cursor = 0
    for s in sents:
        toks = _split_to_tokens(s)
        if not toks:
            continue
        start = cursor
        expected_tokens.extend(toks)
        cursor += len(toks)
        spans.append({"text": s, "tokens": toks, "start": start, "end": cursor})
    return expected_tokens, spans


def gap_ms_between(rec_rows: List[Dict], i: int, j: int) -> Optional[float]:
    try:
        a = rec_rows[i]
        b = rec_rows[j]
        end_a = a.get("end_ms")
        start_b = b.get("offset_ms")
        if isinstance(end_a, (int, float)) and isinstance(start_b, (int, float)):
            g = float(start_b) - float(end_a)
            return max(0.0, g)
    except Exception:
        pass
    return None


def detect_sentence_issues(
    sentence_spans: List[Dict],
    expected_tokens: List[str],
    rec_rows: List[Dict],
    mapping: List[Optional[int]],
    sentence_acc_threshold: float,
    pause_inside_sentence_ms: int,
    min_tokens_for_sentence_check: int,
    max_notes: int = 5,
) -> List[str]:
    notes = []

    for sp in sentence_spans:
        start, end = sp["start"], sp["end"]
        toks = expected_tokens[start:end]
        mapped = mapping[start:end]
        if len(toks) < int(min_tokens_for_sentence_check):
            continue

        missing = sum(1 for x in mapped if x is None)
        found_indices = [x for x in mapped if isinstance(x, int)]
        found_indices_sorted = sorted(found_indices)

        accs = []
        for mi in found_indices_sorted:
            acc = rec_rows[mi].get("accuracy")
            if isinstance(acc, (int, float)):
                accs.append(float(acc))
        avg_acc = (sum(accs) / len(accs)) if accs else None

        long_pause_count = 0
        if len(found_indices_sorted) >= 2:
            for a, b in zip(found_indices_sorted[:-1], found_indices_sorted[1:]):
                g = gap_ms_between(rec_rows, a, b)
                if isinstance(g, (int, float)) and g >= float(pause_inside_sentence_ms):
                    long_pause_count += 1

        flag_pause = long_pause_count > 0
        flag_missing = missing > 0
        flag_acc = isinstance(avg_acc, (int, float)) and avg_acc < float(sentence_acc_threshold)

        if flag_pause or flag_missing or flag_acc:
            reasons = []
            if flag_missing:
                reasons.append(f"thi·∫øu {missing} t·ª´")
            if flag_pause:
                reasons.append(f"ng·∫Øt ngh·ªâ trong c√¢u {long_pause_count} l·∫ßn")
            if flag_acc:
                reasons.append("ƒë·ªô chu·∫©n c·∫£ c√¢u th·∫•p")

            sent_preview = sp["text"]
            if len(sent_preview) > 95:
                sent_preview = sent_preview[:92] + "..."
            notes.append(f'‚Äú{sent_preview}‚Äù ‚Äî ' + ", ".join(reasons))

        if len(notes) >= int(max_notes):
            break

    return notes


def _similarity(a: str, b: str) -> float:
    try:
        return SequenceMatcher(None, a, b).ratio()
    except Exception:
        return 0.0


def _label_from_word_issue(
    expected_tok: str,
    heard_tok: str,
    row: Optional[Dict],
    accuracy_threshold: float,
    s_acc_threshold: float,
    final_acc_threshold: float,
) -> str:
    if row is None:
        return "(thi·∫øu t·ª´)"

    if detect_missing_plural_s(expected_tok, heard_tok):
        return "(thi·∫øu -s s·ªë nhi·ªÅu)"

    has_s, _, s_ph = detect_missing_s_sound(row, s_acc_threshold=float(s_acc_threshold))
    if has_s and s_ph:
        s_ph_norm = s_ph.lower()
        if s_ph_norm == "s":
            return "(thi·∫øu /s/)"
        if s_ph_norm == "z":
            return "(thi·∫øu /z/)"
        return f"(thi·∫øu /{s_ph_norm}/)"

    final_ph = detect_missing_final_sound(row, final_acc_threshold=float(final_acc_threshold))
    if final_ph:
        return f"(/{final_ph}/)"

    etype = (row.get("error_type") or "None").strip()
    acc = row.get("accuracy", None)

    if etype in BAD_TYPES:
        return "(ph√°t √¢m sai)"

    if isinstance(acc, (int, float)) and float(acc) < float(accuracy_threshold):
        return "(ch∆∞a chu·∫©n)"

    return ""


def build_passage_word_issues_concise(
    expected_tokens: List[str],
    rec_rows: List[Dict],
    mapping: List[Optional[int]],
    accuracy_threshold: float,
    s_acc_threshold: float,
    final_acc_threshold: float,
    ignore_stopwords: bool,
    lookahead: int,
    max_items: int = 30,
    suggest_min_ratio: float = 0.82,
) -> List[str]:
    """
    Concise notable issues for passage:
    - Avoid duplicate noise
    - IMPORTANT FIX: If a token appears anywhere as a recognized pronunciation issue,
      do NOT also list it as "(thi·∫øu t·ª´)" later due to mapping drift / pause.
    - IMPORTANT FIX: 'missing final sound' relies on detect_missing_final_sound which now ignores vowel tails.
    """
    stopwords = {
        "a","an","the","to","of","in","on","at","for","with","and","or","but",
        "is","am","are","was","were","be","been","being","do","does","did",
        "i","you","he","she","it","we","they","me","him","her","us","them",
        "my","your","his","her","our","their","this","that","these","those",
        "as","from","by","not","so","if","then","than","too","very"
    }

    rec_tokens = [r.get("tok") or _norm_token(r.get("word", "")) for r in rec_rows]

    # --- PASS 1: collect tokens that are actually recognized with any pronunciation issue
    # This helps us suppress fake "(thi·∫øu t·ª´)" for tokens that were read (but mapping drifted).
    tokens_with_pron_issue = set()

    for r in rec_rows:
        tok = r.get("tok") or _norm_token(r.get("word", ""))
        if not tok:
            continue

        etype = (r.get("error_type") or "None").strip()
        acc = r.get("accuracy", None)

        # Basic pron/low accuracy
        if etype in BAD_TYPES:
            tokens_with_pron_issue.add(tok)
            continue
        if isinstance(acc, (int, float)) and float(acc) < float(accuracy_threshold):
            tokens_with_pron_issue.add(tok)
            continue

        # Missing s/z phoneme
        has_s, _, _ = detect_missing_s_sound(r, s_acc_threshold=float(s_acc_threshold))
        if has_s:
            tokens_with_pron_issue.add(tok)
            continue

        # Missing final sound (now consonant-only)
        final_ph = detect_missing_final_sound(r, final_acc_threshold=float(final_acc_threshold))
        if final_ph:
            tokens_with_pron_issue.add(tok)
            continue

    out: List[str] = []
    seen = set()

    jptr = 0
    n = len(rec_tokens)

    for et, mi in zip(expected_tokens, mapping):
        # stopwords filter
        if ignore_stopwords and et in stopwords:
            if isinstance(mi, int):
                jptr = max(jptr, mi + 1)
            continue

        # CASE A: missing mapping
        if mi is None:
            # FIX: suppress missing if we already have this token as a real pron issue somewhere
            # (e.g., 'cream' has (nu·ªët √¢m cu·ªëi /m/) so don't also show 'cream (thi·∫øu t·ª´)')
            if et in tokens_with_pron_issue:
                continue

            key = ("missing", et)
            if key not in seen:
                out.append(f"{et} (thi·∫øu t·ª´)")
                seen.add(key)

        # CASE B: mapped to recognized word
        else:
            row = rec_rows[mi]
            heard = (row.get("tok") or _norm_token(row.get("word", "")))
            label = _label_from_word_issue(
                expected_tok=et,
                heard_tok=heard,
                row=row,
                accuracy_threshold=float(accuracy_threshold),
                s_acc_threshold=float(s_acc_threshold),
                final_acc_threshold=float(final_acc_threshold),
            )

            # Determine if we should include this token as a notable issue
            etype = (row.get("error_type") or "None").strip()
            acc = row.get("accuracy", None)

            is_issue = False
            if label:
                is_issue = True
            else:
                if etype in BAD_TYPES:
                    is_issue = True
                if isinstance(acc, (int, float)) and float(acc) < float(accuracy_threshold):
                    is_issue = True

            if is_issue:
                # Keep only one entry per expected token to avoid spam
                key = ("bad", et)
                if key not in seen:
                    out.append(f"{et} {label}".strip())
                    seen.add(key)

            jptr = max(jptr, mi + 1)

        if len(out) >= int(max_items):
            break

    return out

# =========================
# SNAPSHOT SAVE (NEW)
# =========================
def save_text(path: str, text: str) -> None:
    os.makedirs(os.path.dirname(path), exist_ok=True)
    with open(path, "w", encoding="utf-8") as f:
        f.write(text or "")


def save_bytes(path: str, data: bytes) -> None:
    os.makedirs(os.path.dirname(path), exist_ok=True)
    with open(path, "wb") as f:
        f.write(data or b"")


def append_log_json(session_dir: str, payload: Dict[str, Any]) -> None:
    os.makedirs(session_dir, exist_ok=True)
    log_path = os.path.join(session_dir, "logs.json")
    try:
        old = []
        if os.path.exists(log_path):
            with open(log_path, "r", encoding="utf-8") as f:
                old = json.load(f) or []
        if not isinstance(old, list):
            old = []
        old.append(payload)
        with open(log_path, "w", encoding="utf-8") as f:
            json.dump(old, f, ensure_ascii=False, indent=2)
    except Exception:
        pass


# =========================
# SIDEBAR (shared config)
# =========================
st.sidebar.header("Azure + QC (d√πng chung cho 2 tab)")

# ---- Class Session (L·ªõp -> Bu·ªïi h·ªçc) ----
st.sidebar.divider()
st.sidebar.header("Class Session (L·ªõp ‚Üí Bu·ªïi h·ªçc)")

def _slugify(s: str) -> str:
    s = (s or "").strip()
    s = re.sub(r"\s+", " ", s)
    s = s.replace("/", "-").replace("\\", "-")
    s = re.sub(r"[^0-9A-Za-z _\-\(\)\.]", "", s)
    s = s.strip().replace(" ", "_")
    return s or "untitled"


SESSIONS_ROOT = os.getenv("SESSIONS_ROOT", "sessions")


def _list_dirs(path: str) -> List[str]:
    try:
        if not os.path.isdir(path):
            return []
        items = []
        for name in os.listdir(path):
            p = os.path.join(path, name)
            if os.path.isdir(p):
                items.append(name)
        items.sort()
        return items
    except Exception:
        return []


existing_classes = _list_dirs(SESSIONS_ROOT)

class_mode = st.sidebar.radio("Class mode", ["Ch·ªçn c√≥ s·∫µn", "T·∫°o m·ªõi"], horizontal=True, key="class_mode")

if class_mode == "Ch·ªçn c√≥ s·∫µn" and existing_classes:
    chosen_class_raw = st.sidebar.selectbox("Ch·ªçn l·ªõp", existing_classes, index=0, key="class_pick")
else:
    chosen_class_raw = st.sidebar.text_input("Nh·∫≠p t√™n l·ªõp (vd: G7)", value="", placeholder="G7", key="class_new")

chosen_class = _slugify(chosen_class_raw)
class_path = os.path.join(SESSIONS_ROOT, chosen_class)
existing_sessions = _list_dirs(class_path)

session_mode = st.sidebar.radio("Session mode", ["Ch·ªçn c√≥ s·∫µn", "T·∫°o m·ªõi"], horizontal=True, key="sess_mode")

if session_mode == "Ch·ªçn c√≥ s·∫µn" and existing_sessions:
    chosen_session_raw = st.sidebar.selectbox("Ch·ªçn bu·ªïi h·ªçc", existing_sessions, index=len(existing_sessions) - 1, key="sess_pick")
else:
    chosen_session_raw = st.sidebar.text_input("Nh·∫≠p bu·ªïi h·ªçc (vd: Week_3)", value="", placeholder="Week_3", key="sess_new")

chosen_session = _slugify(chosen_session_raw)
SESSION_DIR = os.path.join(SESSIONS_ROOT, chosen_class, chosen_session)

st.sidebar.caption(f"üìÅ Session folder: `{SESSION_DIR}`")

if st.sidebar.button("üìå T·∫°o / Load session", use_container_width=True, key="btn_make_session"):
    os.makedirs(SESSION_DIR, exist_ok=True)
    st.sidebar.success("Session ready ‚úÖ")

# Auto ensure folder exists when user already typed class/session (avoid save errors)
if chosen_class and chosen_session:
    try:
        os.makedirs(SESSION_DIR, exist_ok=True)
    except Exception:
        pass

# ---- Azure key/region ----
speech_key_ui = st.sidebar.text_input("SPEECH_KEY", type="password", value="")
speech_region_ui = st.sidebar.text_input("SPEECH_REGION", value=os.getenv("SPEECH_REGION", "southeastasia"))

SPEECH_KEY = (speech_key_ui.strip() or os.getenv("SPEECH_KEY", "").strip())
SPEECH_REGION = (speech_region_ui.strip() or os.getenv("SPEECH_REGION", "").strip())

st.sidebar.divider()
st.sidebar.subheader("üéß Ki·ªÉm tra ch·∫•t l∆∞·ª£ng audio")
st.sidebar.caption("Lo·∫°i audio qu√° ng·∫Øn, qu√° nh·ªè ho·∫∑c ng·∫Øt ngh·ªâ qu√° nhi·ªÅu (khuy·∫øn ngh·ªã b·∫≠t).")

enable_audio_qc = st.sidebar.checkbox("B·∫≠t ki·ªÉm tra audio", value=True)

min_duration_s = st.sidebar.slider(
    "Th·ªùi l∆∞·ª£ng t·ªëi thi·ªÉu (gi√¢y)",
    1.0, 25.0, 6.0, step=0.5,
    disabled=not enable_audio_qc,
    help="N·∫øu audio ng·∫Øn h∆°n m·ª©c n√†y ‚Üí s·∫Ω kh√¥ng ch·∫•m v√† y√™u c·∫ßu thu l·∫°i."
)

min_dbfs = st.sidebar.slider(
    "√Çm l∆∞·ª£ng t·ªëi thi·ªÉu",
    -60.0, -10.0, -35.0, step=1.0,
    disabled=not enable_audio_qc,
    help="N·∫øu h·ªçc sinh n√≥i qu√° nh·ªè ‚Üí h·ªá th·ªëng d·ªÖ nh·∫≠n sai. D∆∞·ªõi ng∆∞·ª°ng s·∫Ω b·ªã ch·∫∑n."
)

max_silence_ratio = st.sidebar.slider(
    "T·ª∑ l·ªá im l·∫∑ng t·ªëi ƒëa",
    0.10, 0.95, 0.65, step=0.05,
    disabled=not enable_audio_qc,
    help="N·∫øu t·ª∑ l·ªá im l·∫∑ng qu√° cao (ng·∫Øt ngh·ªâ qu√° nhi·ªÅu) ‚Üí ch·∫∑n ƒë·ªÉ tr√°nh k·∫øt qu·∫£ sai l·ªách."
)

min_silence_len_ms = st.sidebar.slider(
    "Kho·∫£ng im l·∫∑ng t·ªëi thi·ªÉu (ms)",
    200, 1500, 500, step=50,
    disabled=not enable_audio_qc,
    help="ƒêo·∫°n im l·∫∑ng ng·∫Øn h∆°n m·ª©c n√†y s·∫Ω kh√¥ng t√≠nh l√† 'im l·∫∑ng'."
)

silence_rel_db = st.sidebar.slider(
    "ƒê·ªô nh·∫°y ph√°t hi·ªán im l·∫∑ng",
    8.0, 25.0, 16.0, step=1.0,
    disabled=not enable_audio_qc,
    help="S·ªë c√†ng nh·ªè ‚Üí b·∫Øt im l·∫∑ng 'nh·∫°y' h∆°n (d·ªÖ t√≠nh l√† im l·∫∑ng). S·ªë c√†ng l·ªõn ‚Üí √≠t nh·∫°y h∆°n."
)

long_pause_ms = st.sidebar.slider(
    "Ng·∫Øt ngh·ªâ d√†i (ms)",
    600, 2000, 1000, step=100,
    disabled=not enable_audio_qc,
    help="ƒê·∫øm s·ªë l·∫ßn h·ªçc sinh ng·ª´ng qu√° l√¢u. D√πng ƒë·ªÉ tham kh·∫£o khi ƒë√°nh gi√° t·ªëc ƒë·ªô/nh·ªãp ƒë·ªçc."
)

st.sidebar.divider()
st.sidebar.subheader("Timeouts (Azure)")
auto_tune_pause = st.sidebar.checkbox("T·ª± ch·ªânh timeout theo ƒë·ªô d√†i", value=True)
pause_profile = st.sidebar.selectbox("M·ª©c ng·∫Øt ngh·ªâ", ["Nh·∫π", "V·ª´a", "Nhi·ªÅu"], index=1, disabled=not auto_tune_pause)
end_silence_timeout_ms = st.sidebar.slider("EndSilenceTimeoutMs", 1000, 20000, 16000, step=500)
seg_silence_timeout_ms = st.sidebar.slider("SegmentationSilenceTimeoutMs", 300, 5000, 2800, step=100)
initial_silence_timeout_ms = st.sidebar.slider("InitialSilenceTimeoutMs", 2000, 30000, 15000, step=500)
locale = st.sidebar.selectbox("Locale", ["en-US", "en-GB", "en-AU"], index=0)

if locale != "en-US":
    st.sidebar.caption("‚ö†Ô∏è Prosody assessment ch·ªâ h·ªó tr·ª£ en-US (American English). T·ª± ƒë·ªông t·∫Øt.")
    enable_prosody = False
else:
    enable_prosody = st.sidebar.checkbox("Enable Prosody", value=True)

# ---- History download ----
st.sidebar.divider()
st.sidebar.subheader("History (t·∫£i snapshot)")


def _dl_button_if_exists(label: str, file_path: str, mime: str):
    if os.path.exists(file_path):
        try:
            with open(file_path, "rb") as f:
                st.sidebar.download_button(
                    label,
                    f.read(),
                    file_name=os.path.basename(file_path),
                    mime=mime,
                    use_container_width=True,
                )
        except Exception:
            pass


if chosen_class and chosen_session and os.path.isdir(SESSION_DIR):
    _dl_button_if_exists("‚¨áÔ∏è vocab_tableA_summary.csv", os.path.join(SESSION_DIR, "vocab_tableA_summary.csv"), "text/csv")
    _dl_button_if_exists("‚¨áÔ∏è vocab_summary.txt", os.path.join(SESSION_DIR, "vocab_summary.txt"), "text/plain")
    _dl_button_if_exists("‚¨áÔ∏è passage_level1_summary.csv", os.path.join(SESSION_DIR, "passage_level1_summary.csv"), "text/csv")
    _dl_button_if_exists("‚¨áÔ∏è passage_summary.txt", os.path.join(SESSION_DIR, "passage_summary.txt"), "text/plain")
    _dl_button_if_exists("‚¨áÔ∏è logs.json", os.path.join(SESSION_DIR, "logs.json"), "application/json")
else:
    st.sidebar.caption("Ch·ªçn l·ªõp + bu·ªïi h·ªçc ƒë·ªÉ hi·ªán file ƒë√£ l∆∞u.")

# === M·ªöI: T√°ch ri√™ng aggregator cho t·ª´ng tab ===
if "top_issues_vocab" not in st.session_state:
    st.session_state["top_issues_vocab"] = {}       # D√†nh ri√™ng cho Tab 1 (Vocab/Phrase)

if "top_issues_passage" not in st.session_state:
    st.session_state["top_issues_passage"] = {}     # D√†nh ri√™ng cho Tab 2 (Reading Passage)

# =========================
# CLASS-WIDE TOP 10 ISSUES (Tab1 + Tab2)
# =========================
def _extract_issue_key(issue_text: str) -> Optional[str]:
    """
    Ch·ªâ tr√≠ch xu·∫•t t·ª´ g·ªëc (word) ƒë·ªÉ gom nh√≥m top t·ª´ b·ªã sai nhi·ªÅu nh·∫•t.
    B·ªè ho√†n to√†n lo·∫°i l·ªói v√† prosody.
    V√≠ d·ª•:
    - "look (nu·ªët √¢m cu·ªëi /k/)" ‚Üí "look"
    - "rice (thi·∫øu /s/)" ‚Üí "rice"
    - "fish (ph√°t √¢m sai)" ‚Üí "fish"
    - "im (ph√°t √¢m sai)" ‚Üí "im"
    """
    s = (issue_text or "").strip()
    
    # B·ªè c√°c d√≤ng QC fail ho·∫∑c nhi·ªÖu
    if s.startswith("‚ö†Ô∏è") or s.startswith("‚ùå"):
        return None
    s_low = s.lower()
    if "convert error" in s_low or "qc error" in s_low or ("audio" in s_low and "qc" in s_low):
        return None
    if "prosody" in s_low:
        return None  # B·ªè prosody ra kh·ªèi top
    if not s:
        return None

    # L·∫•y t·ª´ g·ªëc: ph·∫ßn ƒë·∫ßu ti√™n tr∆∞·ªõc kho·∫£ng tr·∫Øng ho·∫∑c ngo·∫∑c
    # V√≠ d·ª•: "look (nu·ªët √¢m cu·ªëi /k/)" ‚Üí "look"
    #       "im (ph√°t √¢m sai)" ‚Üí "im"
    match = re.match(r"^(\S+)", s)
    if match:
        word = match.group(1).strip()
        # Chu·∫©n h√≥a v·ªÅ lowercase ƒë·ªÉ gom ch√≠nh x√°c (look v√† Look l√† m·ªôt)
        return word.lower()
    
        return None
    
def _agg_add(agg: Dict[str, set], student: str, issues: List[str]) -> None:
    """
    agg: dict key -> set(students)
    Each key counted at most once per student.
    """
    st_name = (student or "").strip()
    if not st_name:
        return

    for it in issues or []:
        k = _extract_issue_key(it)
        if not k:
            continue
        if k not in agg:
            agg[k] = set()
        agg[k].add(st_name)

def _render_top10_issues(agg: Dict[str, set], title: str = "10 t·ª´ b·ªã sai ph·ªï bi·∫øn nh·∫•t trong l·ªõp", top_n: int = 10) -> None:
    if not agg:
        st.info("Ch∆∞a c√≥ d·ªØ li·ªáu ƒë·ªÉ t·ªïng h·ª£p (h√£y ch·∫•m √≠t nh·∫•t 1 lo·∫°t b√†i).")
        return

    # Chuy·ªÉn sang list v√† sort theo s·ªë h·ªçc sinh gi·∫£m d·∫ßn, r·ªìi alphabet
    items = [(k.capitalize(), sorted(list(v))) for k, v in agg.items() if v]
    items.sort(key=lambda x: (-len(x[1]), x[0].lower()))

    st.subheader(title)
    for rank, (word, students) in enumerate(items[:top_n], start=1):
        st.markdown(f"**{rank}. {word}** ‚Äî {', '.join(students)}")

# =========================
# TABS
# =========================
tab_vocab, tab_passage = st.tabs(["1) Ch·∫•m vocab/phrase (Table A)", "2) Ch·∫•m ƒë·ªçc ƒëo·∫°n vƒÉn (Passage Level 1)"])

# =============================================================================
# TAB 1 ‚Äî VOCAB / TABLE A
# =============================================================================
with tab_vocab:
    st.subheader("Tab 1 ‚Äî Ch·∫•m theo danh s√°ch t·ª´/c·ª•m t·ª´ (Table A + Summary)")

    if not has_ffmpeg():
        st.warning("Server ch∆∞a c√≥ ffmpeg. Upload mp3/m4a/webm/ogg c√≥ th·ªÉ convert fail. Khuy·∫øn ngh·ªã upload .wav ho·∫∑c c√†i ffmpeg.")

    colA, colB = st.columns([2, 1], gap="large")

    with colA:
        st.markdown("### 1) Danh s√°ch t·ª´ / c·ª•m t·ª´")
        vocab_text = st.text_area(
            "M·ªói d√≤ng 1 item (word ho·∫∑c phrase)",
            height=220,
            placeholder="confident\ndevelop country\nset off\ntake something for granted",
            key="vocab_text_tab1",
        )

        st.markdown("### 2) Upload audio (nhi·ªÅu file ƒë·ªÉ ch·∫•m c·∫£ l·ªõp)")
        uploaded_files = st.file_uploader(
            "T√™n file n√™n l√† t√™n h·ªçc sinh (vd: AnhNguyet.m4a)",
            type=["mp3", "m4a", "wav", "webm", "ogg"],
            accept_multiple_files=True,
            key="uploader_tab1",
        )

        run_btn = st.button("‚úÖ CH·∫§M VOCAB (Batch)", type="primary", use_container_width=True, key="run_tab1")

    with colB:
        st.markdown("### Ng∆∞·ª°ng ch·∫•m (Tab 1)")

    strictness_t1 = st.slider(
        "Strictness (D·ªÖ ‚Üí Kh·∫Øt khe)",
        0, 100, 55,
        key="t1_strictness",
        help="K√©o sang ph·∫£i = ch·∫•m kh·∫Øt khe h∆°n (d·ªÖ b·∫Øt l·ªói h∆°n)."
    )

    def _lerp(a, b, t):
        return a + (b - a) * t

    t1 = strictness_t1 / 100.0
    t1_default_acc_thr   = int(round(_lerp(85, 95, t1)))
    t1_default_s_thr     = int(round(_lerp(80, 92, t1)))
    t1_default_final_thr = int(round(_lerp(80, 92, t1)))
    t1_default_pros_thr  = int(round(_lerp(60, 80, t1)))
    t1_default_lookahead = int(round(_lerp(6, 3, t1)))

    with st.expander("Advanced (tu·ª≥ ch·ªânh chi ti·∫øt)", expanded=False):
        st.caption("C√°c gi√° tr·ªã ·ªü ƒë√¢y s·∫Ω override Strictness.")
        t1_acc_thr_adv = st.slider("Low accuracy (Accuracy <)", 0, 100, t1_default_acc_thr, key="t1_acc_thr_adv")
        t1_s_thr_adv = st.slider("B·∫Øt thi·∫øu /s/~/z/ (ng∆∞·ª°ng)", 0, 100, t1_default_s_thr, key="t1_s_thr_adv")
        t1_final_thr_adv = st.slider("B·∫Øt nu·ªët √¢m cu·ªëi (ng∆∞·ª°ng)", 0, 100, t1_default_final_thr, key="t1_final_thr_adv")
        t1_pros_thr_adv = st.slider("Ng∆∞·ª°ng Prosody warning", 0, 100, t1_default_pros_thr, key="t1_pros_thr_adv")
        t1_lookahead_adv = st.slider("Lookahead (ch·ªãu token nhi·ªÖu)", 1, 8, t1_default_lookahead, key="t1_lookahead_adv")

    # Final values used by Tab 1
    accuracy_threshold = t1_acc_thr_adv
    s_acc_threshold = t1_s_thr_adv
    final_acc_threshold = t1_final_thr_adv
    prosody_threshold = t1_pros_thr_adv
    lookahead = t1_lookahead_adv

    st.divider()
    st.markdown("### Score (%)")
    missing_token_score = st.slider("ƒêi·ªÉm cho t·ª´ b·ªã thi·∫øu", 0.0, 50.0, 0.0, step=1.0, key="t1_missing_score")

    st.divider()
    st.markdown("### Worst-per-word summary")
    worst_limit = st.slider("Gi·ªõi h·∫°n s·ªë l·ªói trong summary", 10, 120, 60, step=5, key="t1_worst_limit")

    st.divider()
    st.markdown("### Ghi ch√∫")
    st.caption(
            "Table A l√† ƒë·ªÉ ph√¢n t√≠ch chi ti·∫øt theo lo·∫°i l·ªói. "
            "Summary b√™n d∆∞·ªõi s·∫Ω t·ª± gom v√† ch·ªâ gi·ªØ l·ªói n·∫∑ng nh·∫•t cho m·ªói t·ª´/phrase (tr√°nh l·∫∑p)."
        )

    st.markdown(
        """
#### Quy ∆∞·ªõc Table A
- **Ph√°t √¢m sai/ƒëi·ªÉm th·∫•p**: Azure b√°o Mispronunciation/Omission/Insertion ho·∫∑c Accuracy d∆∞·ªõi ng∆∞·ª°ng.
- **Nu·ªët √¢m cu·ªëi**: suy lu·∫≠n t·ª´ phoneme tail y·∫øu (best-effort).
- **Thi·∫øu/ y·∫øu /s/~/z/**: b·∫Øt theo phoneme /s/ ho·∫∑c /z/ y·∫øu (best-effort).
- **Thi·∫øu -s s·ªë nhi·ªÅu**: expected c√≥ 's' nh∆∞ng Azure nghe ra d·∫°ng kh√¥ng 's' (heuristic).
- **Prosody**: ch·ªâ l√† c·∫£nh b√°o (best-effort).
"""
    )

    if run_btn:
        if not vocab_text.strip():
            st.error("B·∫°n ch∆∞a d√°n danh s√°ch t·ª´.")
            st.stop()
        if not uploaded_files:
            st.error("B·∫°n ch∆∞a upload audio.")
            st.stop()
        if not SPEECH_KEY or not SPEECH_REGION:
            st.error("Thi·∫øu SPEECH_KEY ho·∫∑c SPEECH_REGION (nh·∫≠p ·ªü sidebar).")
            st.stop()

        vocab_list = [line.strip() for line in vocab_text.splitlines() if line.strip()]
        if not vocab_list:
            st.error("Danh s√°ch r·ªóng.")
            st.stop()

        reference_text = " ".join(vocab_list)

        expected_tokens_vocab = []
        for item in vocab_list:
            expected_tokens_vocab.extend(_item_tokens(item))

        st.divider()
        st.subheader("üìä Table A ‚Äî B·∫£ng t·ªïng k·∫øt theo h·ªçc sinh")

        summary_placeholder = st.empty()
        download_placeholder = st.empty()

        progress = st.progress(0)
        status_line = st.empty()

        table_rows: List[Dict] = []
        summary_lines: List[str] = []

        for idx, up in enumerate(uploaded_files):
            status_line.write(f"ƒêang ch·∫•m: **{up.name}** ({idx+1}/{len(uploaded_files)})")
            progress.progress(int(((idx + 1) / max(1, len(uploaded_files))) * 100))

            student = guess_student_name(up.name)

            with tempfile.TemporaryDirectory() as tmp_dir:
                suffix = Path(up.name).suffix.lower() or ".wav"
                src_path = os.path.join(tmp_dir, f"input{suffix}")
                wav_path = os.path.join(tmp_dir, "input_16k_mono.wav")

                with open(src_path, "wb") as f:
                    f.write(up.getbuffer())

                # Convert
                try:
                    duration_ms, _ = to_wav_16k_mono(src_path, wav_path)
                except Exception as e:
                    table_rows.append(
                        {
                            "H·ªçc sinh": student,
                            "Audio QC": f"‚ùå Convert error: {e}",
                            "Score (%)": "",
                            "Missing words": "",
                            "Ph√°t √¢m sai/ƒëi·ªÉm th·∫•p": "",
                            "Nu·ªët √¢m cu·ªëi": "",
                            "Thi·∫øu/ y·∫øu /s/~/z/": "",
                            "Thi·∫øu -s s·ªë nhi·ªÅu": "",
                            "Prosody": "",
                            "Omission (kh√¥ng match)": "",
                        }
                    )
                    continue

                # QC gate
                qc_msg = ""
                metrics = {}
                if enable_audio_qc:
                    try:
                        metrics, issues = analyze_audio_quality(
                            wav_path=wav_path,
                            min_duration_s=float(min_duration_s),
                            min_dbfs=float(min_dbfs),
                            max_silence_ratio=float(max_silence_ratio),
                            min_silence_len_ms=int(min_silence_len_ms),
                            silence_rel_db=float(silence_rel_db),
                            long_pause_ms=int(long_pause_ms),
                        )
                        if issues:
                            qc_msg = "‚ö†Ô∏è " + " | ".join(issues)
                            table_rows.append(
                                {
                                    "H·ªçc sinh": student,
                                    "Audio QC": qc_msg,
                                    "Score (%)": "",
                                    "Missing words": "",
                                    "Ph√°t √¢m sai/ƒëi·ªÉm th·∫•p": "",
                                    "Nu·ªët √¢m cu·ªëi": "",
                                    "Thi·∫øu/ y·∫øu /s/~/z/": "",
                                    "Thi·∫øu -s s·ªë nhi·ªÅu": "",
                                    "Prosody": "",
                                    "Omission (kh√¥ng match)": "",
                                }
                            )
                            continue
                        qc_msg = f"‚úÖ OK (dur {metrics['duration_s']:.1f}s, {metrics['dbfs']:.1f} dBFS, silence {metrics['silence_ratio']:.0%})"
                    except Exception as e:
                        qc_msg = f"‚ö†Ô∏è QC error: {e}"

                dur_s = max(1, int(duration_ms / 1000))

                # Auto-tune timeouts
                _end = int(end_silence_timeout_ms)
                _seg = int(seg_silence_timeout_ms)
                _init = int(initial_silence_timeout_ms)

                if auto_tune_pause:
                    if pause_profile == "Nh·∫π":
                        seg, end, buffer = 1800, 12000, 35
                    elif pause_profile == "V·ª´a":
                        seg, end, buffer = 2800, 16000, 50
                    else:
                        seg, end, buffer = 4200, 20000, 70

                    if dur_s >= 90:
                        seg = min(5000, seg + 700)
                        end = min(20000, end + 2000)
                        buffer += 20
                    elif dur_s <= 20:
                        seg = max(1200, seg - 500)
                        end = max(10000, end - 2000)

                    _seg, _end = int(seg), int(end)
                    _init = max(int(_init), 15000)
                    max_wait_seconds = min(600, max(60, dur_s + buffer))
                else:
                    max_wait_seconds = max(60, min(dur_s + 30, 600))

                # Azure call
                try:
                    results_list = call_with_retry(
                        lambda: run_pron_assessment_continuous(
                            wav_path=wav_path,
                            reference_text=reference_text,
                            locale=locale,
                            speech_key=SPEECH_KEY,
                            speech_region=SPEECH_REGION,
                            end_silence_timeout_ms=_end,
                            seg_silence_timeout_ms=_seg,
                            initial_silence_timeout_ms=_init,
                            max_wait_seconds=max_wait_seconds,
                            enable_prosody=bool(enable_prosody),
                        ),
                        max_retries=3,
                        base_sleep=1.0,
                    )
                except Exception as e:
                    table_rows.append(
                        {
                            "H·ªçc sinh": student,
                            "Audio QC": qc_msg,
                            "Score (%)": "",
                            "Missing words": "",
                            "Ph√°t √¢m sai/ƒëi·ªÉm th·∫•p": f"‚ùå Azure error: {e}",
                            "Nu·ªët √¢m cu·ªëi": "",
                            "Thi·∫øu/ y·∫øu /s/~/z/": "",
                            "Thi·∫øu -s s·ªë nhi·ªÅu": "",
                            "Prosody": "",
                            "Omission (kh√¥ng match)": "",
                        }
                    )
                    continue

                if not results_list:
                    table_rows.append(
                        {
                            "H·ªçc sinh": student,
                            "Audio QC": qc_msg,
                            "Score (%)": "",
                            "Missing words": "",
                            "Ph√°t √¢m sai/ƒëi·ªÉm th·∫•p": "‚ö†Ô∏è Kh√¥ng nh·∫≠n di·ªán ƒë∆∞·ª£c segment",
                            "Nu·ªët √¢m cu·ªëi": "",
                            "Thi·∫øu/ y·∫øu /s/~/z/": "",
                            "Thi·∫øu -s s·ªë nhi·ªÅu": "",
                            "Prosody": "",
                            "Omission (kh√¥ng match)": "",
                        }
                    )
                    continue

                all_rows: List[Dict] = []
                for rj in results_list:
                    all_rows.extend(extract_word_rows(rj))

                rec_rows = [r for r in all_rows if (r.get("tok") or "").strip()]
                if not rec_rows:
                    table_rows.append(
                        {
                            "H·ªçc sinh": student,
                            "Audio QC": qc_msg,
                            "Score (%)": "",
                            "Missing words": "",
                            "Ph√°t √¢m sai/ƒëi·ªÉm th·∫•p": "‚ö†Ô∏è Kh√¥ng c√≥ Words trong JSON",
                            "Nu·ªët √¢m cu·ªëi": "",
                            "Thi·∫øu/ y·∫øu /s/~/z/": "",
                            "Thi·∫øu -s s·ªë nhi·ªÅu": "",
                            "Prosody": "",
                            "Omission (kh√¥ng match)": "",
                        }
                    )
                    continue

                # Score (%)
                score_pack = compute_word_based_score(
                    expected_tokens=expected_tokens_vocab,
                    rec_rows=rec_rows,
                    lookahead=int(lookahead),
                    missing_token_score=float(missing_token_score),
                )

                # Table A buckets + structured issues
                buckets, issues_struct = build_student_error_buckets_and_issues(
                    vocab_list=vocab_list,
                    all_rows=all_rows,
                    lookahead=int(lookahead),
                    accuracy_threshold=float(accuracy_threshold),
                    s_acc_threshold=float(s_acc_threshold),
                    final_acc_threshold=float(final_acc_threshold),
                    enable_prosody=bool(enable_prosody),
                    prosody_threshold=float(prosody_threshold),
                )

                def join_cell(xs: List[str]) -> str:
                    if not xs:
                        return ""
                    return " | ".join(xs)

                table_rows.append(
                    {
                        "H·ªçc sinh": student,
                        "Audio QC": qc_msg,
                        "Score (%)": f"{score_pack['score_pct']:.1f}" if isinstance(score_pack.get("score_pct"), (int, float)) else "",
                        "Missing words": f"{score_pack['missing']}/{score_pack['total']}" if score_pack.get("total") else "",
                        "Ph√°t √¢m sai/ƒëi·ªÉm th·∫•p": join_cell(buckets["mispron_low"]),
                        "Nu·ªët √¢m cu·ªëi": join_cell(buckets["missing_final"]),
                        "Thi·∫øu/ y·∫øu /s/~/z/": join_cell(buckets["missing_s"]),
                        "Thi·∫øu -s s·ªë nhi·ªÅu": join_cell(buckets["missing_plural_s"]),
                        "Prosody": join_cell(buckets["prosody"]),
                        "Omission (kh√¥ng match)": join_cell(buckets["omission"]),
                    }
                )

                worst_list = summarize_worst_per_word(issues_struct, limit=int(worst_limit))

                score_txt = ""
                if isinstance(score_pack.get("score_pct"), (int, float)):
                    score_txt = f" (Score {score_pack['score_pct']:.1f}%)"

                if worst_list:
                 summary_lines.append(f"{len(summary_lines)+1}. {student}{score_txt}: " + " | ".join(worst_list))
                else:
                    summary_lines.append(f"{len(summary_lines)+1}. {student}{score_txt}: (Kh√¥ng c√≥ l·ªói ƒë√°ng ch√∫ √Ω)")

                # Update global Top-issues aggregator (Tab 1)
                if worst_list:
                    _agg_add(st.session_state["top_issues_agg"], student, worst_list)

            # live update
            if pd is not None:
                summary_placeholder.dataframe(pd.DataFrame(table_rows), use_container_width=True)

        status_line.write("‚úÖ Xong!")

        # Render final table + export
        csv_bytes = b""
        if pd is not None:
            df = pd.DataFrame(table_rows)
            summary_placeholder.dataframe(df, use_container_width=True)

            csv_bytes = df.to_csv(index=False).encode("utf-8-sig")
            download_placeholder.download_button(
                "‚¨áÔ∏è T·∫£i CSV Table A (Vocab)",
                data=csv_bytes,
                file_name="vocab_tableA_summary.csv",
                mime="text/csv",
                use_container_width=True,
            )

        # Text summary under table
        st.divider()
        st.subheader("üìå T√≥m t·∫Øt l·ªói theo t·ª´ng h·ªçc sinh (worst-per-word) ‚Äî ƒë·ªÉ copy/paste g·ª≠i h·ªçc sinh")
        full_text = "\n".join(summary_lines)

        # --------- SAVE SNAPSHOT (Tab 1) ----------
        try:
            os.makedirs(SESSION_DIR, exist_ok=True)
            if csv_bytes:
                save_bytes(os.path.join(SESSION_DIR, "vocab_tableA_summary.csv"), csv_bytes)
            save_text(os.path.join(SESSION_DIR, "vocab_summary.txt"), full_text)

            append_log_json(SESSION_DIR, {
                "type": "vocab",
                "class": chosen_class,
                "session": chosen_session,
                "locale": locale,
                "files_count": len(uploaded_files) if uploaded_files else 0,
                "saved_at": time.strftime("%Y-%m-%d %H:%M:%S"),
            })
        except Exception as e:
            st.warning(f"Snapshot save warning (Tab 1): {e}")

        st.text_area("Copy nhanh (Ctrl+A ‚Üí Ctrl+C)", value=full_text, height=240, key="t1_summary_text")

        st.markdown("#### Preview")
        for line in summary_lines:
            st.markdown(line)
        st.divider()
        st.subheader("üî§ Top 10 l·ªói ph·ªï bi·∫øn trong bu·ªïi h·ªçc n√†y ‚Äî Vocab/Phrase (Table A)")

        if st.session_state["top_issues_vocab"]:
            _render_top10_issues(
                st.session_state["top_issues_vocab"],
                title="",  # Kh√¥ng c·∫ßn title n·ªØa v√¨ ƒë√£ c√≥ subheader
                top_n=10
            )
        else:
            st.info("Ch∆∞a c√≥ d·ªØ li·ªáu l·ªói t·ª´ Tab Vocab trong session n√†y.")

# =============================================================================
# TAB 2 ‚Äî PASSAGE LEVEL 1
# =============================================================================
with tab_passage:
    st.subheader("Tab 2 ‚Äî Ch·∫•m ƒë·ªçc ƒëo·∫°n vƒÉn (Level 1)")

    if not has_ffmpeg():
        st.warning("Server ch∆∞a c√≥ ffmpeg. Upload mp3/m4a/webm/ogg c√≥ th·ªÉ convert fail. Khuy·∫øn ngh·ªã upload .wav ho·∫∑c c√†i ffmpeg.")

    colL, colR = st.columns([2, 1], gap="large")
    with colL:
        passage_text = st.text_area(
            "D√°n ƒëo·∫°n vƒÉn m·∫´u (reference text)",
            height=220,
            placeholder="Paste your reading passage here...",
            key="passage_text_tab2",
        )
        uploaded_files_passage = st.file_uploader(
            "Upload audio (nhi·ªÅu file) ‚Äî t√™n file l√† t√™n h·ªçc sinh",
            type=["mp3", "m4a", "wav", "webm", "ogg"],
            accept_multiple_files=True,
            key="uploader_tab2",
        )
        run_passage = st.button("‚úÖ CH·∫§M PASSAGE (Batch)", type="primary", use_container_width=True, key="run_tab2")

    with colR:
        st.markdown("### Ch·∫•m ƒëi·ªÉm & Notable issues (Tab 2)")

    strictness_t2 = st.slider(
        "Strictness (D·ªÖ ‚Üí Kh·∫Øt khe)",
        0, 100, 55,
        key="t2_strictness",
        help="K√©o sang ph·∫£i = ch·∫•m kh·∫Øt khe h∆°n (d·ªÖ b·∫Øt l·ªói h∆°n)."
    )

    def _lerp(a, b, t):
        return a + (b - a) * t

    t2 = strictness_t2 / 100.0
    t2_default_acc_thr   = int(round(_lerp(80, 92, t2)))
    t2_default_s_thr     = int(round(_lerp(78, 90, t2)))
    t2_default_final_thr = int(round(_lerp(78, 90, t2)))
    t2_default_lookahead = int(round(_lerp(6, 3, t2)))
    t2_default_max_word  = int(round(_lerp(35, 20, t2)))

    with st.expander("Advanced (tu·ª≥ ch·ªânh chi ti·∫øt)", expanded=False):
        st.caption("C√°c gi√° tr·ªã ·ªü ƒë√¢y s·∫Ω override Strictness.")

        lookahead_p = st.slider("Lookahead (ch·ªãu nhi·ªÖu)", 1, 8, t2_default_lookahead, key="t2_lookahead_adv")
        missing_token_score_p = st.slider("ƒêi·ªÉm cho t·ª´ b·ªã thi·∫øu", 0.0, 50.0, 0.0, step=1.0, key="t2_missing_score_adv")

        accuracy_threshold_p = st.slider("Ng∆∞·ª°ng low accuracy", 0, 100, t2_default_acc_thr, key="t2_acc_thr_adv")
        s_acc_threshold_p = st.slider("Ng∆∞·ª°ng b·∫Øt thi·∫øu /s/~/z/", 0, 100, t2_default_s_thr, key="t2_s_thr_adv")
        final_acc_threshold_p = st.slider("Ng∆∞·ª°ng b·∫Øt nu·ªët √¢m cu·ªëi", 0, 100, t2_default_final_thr, key="t2_final_thr_adv")

        ignore_stopwords = st.checkbox("Gi·∫£m nhi·ªÖu: b·ªè stopwords", value=True, key="t2_stop_adv")
        max_word_issues = st.slider("Max notable word issues", 5, 80, t2_default_max_word, step=1, key="t2_max_word_adv")

        st.divider()
        st.markdown("### Sentence-level notes (c√¢u d√†i sai)")
        min_tokens_for_sentence_check = st.slider("Ch·ªâ check c√¢u >= (tokens)", 5, 30, 10, step=1, key="t2_min_tok_sent")
        pause_inside_sentence_ms = st.slider("Ng·∫Øt ngh·ªâ trong c√¢u > (ms)", 300, 2000, 800, step=50, key="t2_pause_in_sent")
        sentence_acc_threshold = st.slider("Ng∆∞·ª°ng avg accuracy theo c√¢u", 0, 100, 80, key="t2_sent_acc")
        max_sentence_notes = st.slider("Max sentence notes", 1, 12, 5, step=1, key="t2_max_sent_notes")

    if run_passage:
        if not passage_text.strip():
            st.error("B·∫°n ch∆∞a d√°n ƒëo·∫°n vƒÉn m·∫´u.")
            st.stop()
        if not uploaded_files_passage:
            st.error("B·∫°n ch∆∞a upload audio.")
            st.stop()
        if not SPEECH_KEY or not SPEECH_REGION:
            st.error("Thi·∫øu SPEECH_KEY ho·∫∑c SPEECH_REGION (nh·∫≠p ·ªü sidebar).")
            st.stop()

        expected_tokens, sentence_spans = build_sentence_spans(passage_text)
        if len(expected_tokens) < 5:
            st.error("ƒêo·∫°n vƒÉn qu√° ng·∫Øn ho·∫∑c kh√¥ng t√°ch ƒë∆∞·ª£c token.")
            st.stop()

        reference_text = passage_text.strip()

        st.divider()
        st.subheader("üìä B·∫£ng k·∫øt qu·∫£ ‚Äî Reading Passage (Level 1)")

        table_rows: List[Dict] = []
        sentence_issues_by_student: Dict[str, List[str]] = {}
        word_issues_by_student: Dict[str, List[str]] = {}

        progress = st.progress(0)
        status = st.empty()

        for idx, up in enumerate(uploaded_files_passage):
            student = guess_student_name(up.name)
            status.write(f"ƒêang ch·∫•m: **{up.name}** ({idx+1}/{len(uploaded_files_passage)})")
            progress.progress(int(((idx + 1) / max(1, len(uploaded_files_passage))) * 100))

            with tempfile.TemporaryDirectory() as tmp_dir:
                suffix = Path(up.name).suffix.lower() or ".wav"
                src_path = os.path.join(tmp_dir, f"input{suffix}")
                wav_path = os.path.join(tmp_dir, "input_16k_mono.wav")

                with open(src_path, "wb") as f:
                    f.write(up.getbuffer())

                try:
                    duration_ms, _ = to_wav_16k_mono(src_path, wav_path)
                except Exception as e:
                    table_rows.append(
                        {
                            "H·ªçc sinh": student,
                            "Audio QC": f"‚ùå Convert error: {e}",
                            "Score (%)": "",
                            "Missing words": "",
                            "WPM": "",
                            "Pause ratio": "",
                            "Long pauses": "",
                            "Sentence issues": "",
                            "Notable issues": "",
                        }
                    )
                    continue

                qc_msg = ""
                metrics = {}
                if enable_audio_qc:
                    try:
                        metrics, issues = analyze_audio_quality(
                            wav_path=wav_path,
                            min_duration_s=float(min_duration_s),
                            min_dbfs=float(min_dbfs),
                            max_silence_ratio=float(max_silence_ratio),
                            min_silence_len_ms=int(min_silence_len_ms),
                            silence_rel_db=float(silence_rel_db),
                            long_pause_ms=int(long_pause_ms),
                        )
                        if issues:
                            qc_msg = "‚ö†Ô∏è " + " | ".join(issues)
                            table_rows.append(
                                {
                                    "H·ªçc sinh": student,
                                    "Audio QC": qc_msg,
                                    "Score (%)": "",
                                    "Missing words": "",
                                    "WPM": "",
                                    "Pause ratio": "",
                                    "Long pauses": "",
                                    "Sentence issues": "",
                                    "Notable issues": "",
                                }
                            )
                            continue
                        qc_msg = f"‚úÖ OK (dur {metrics['duration_s']:.1f}s, {metrics['dbfs']:.1f} dBFS, silence {metrics['silence_ratio']:.0%})"
                    except Exception as e:
                        qc_msg = f"‚ö†Ô∏è QC error: {e}"

                dur_s = max(1, int(duration_ms / 1000))

                _end = int(end_silence_timeout_ms)
                _seg = int(seg_silence_timeout_ms)
                _init = int(initial_silence_timeout_ms)

                if auto_tune_pause:
                    if pause_profile == "Nh·∫π":
                        seg, end, buffer = 1800, 12000, 35
                    elif pause_profile == "V·ª´a":
                        seg, end, buffer = 2800, 16000, 50
                    else:
                        seg, end, buffer = 4200, 20000, 70

                    if dur_s >= 90:
                        seg = min(5000, seg + 700)
                        end = min(20000, end + 2000)
                        buffer += 20
                    elif dur_s <= 20:
                        seg = max(1200, seg - 500)
                        end = max(10000, end - 2000)

                    _seg, _end = int(seg), int(end)
                    _init = max(int(_init), 15000)
                    max_wait_seconds = min(600, max(60, dur_s + buffer))
                else:
                    max_wait_seconds = max(60, min(dur_s + 30, 600))

                try:
                    results_list = call_with_retry(
                        lambda: run_pron_assessment_continuous(
                            wav_path=wav_path,
                            reference_text=reference_text,
                            locale=locale,
                            speech_key=SPEECH_KEY,
                            speech_region=SPEECH_REGION,
                            end_silence_timeout_ms=_end,
                            seg_silence_timeout_ms=_seg,
                            initial_silence_timeout_ms=_init,
                            max_wait_seconds=max_wait_seconds,
                            enable_prosody=bool(enable_prosody),
                        ),
                        max_retries=3,
                        base_sleep=1.0,
                    )
                except Exception as e:
                    table_rows.append(
                        {
                            "H·ªçc sinh": student,
                            "Audio QC": qc_msg,
                            "Score (%)": "",
                            "Missing words": "",
                            "WPM": "",
                            "Pause ratio": "",
                            "Long pauses": "",
                            "Sentence issues": "",
                            "Notable issues": f"‚ùå Azure error: {e}",
                        }
                    )
                    continue

                if not results_list:
                    table_rows.append(
                        {
                            "H·ªçc sinh": student,
                            "Audio QC": qc_msg,
                            "Score (%)": "",
                            "Missing words": "",
                            "WPM": "",
                            "Pause ratio": "",
                            "Long pauses": "",
                            "Sentence issues": "",
                            "Notable issues": "‚ö†Ô∏è Kh√¥ng nh·∫≠n di·ªán ƒë∆∞·ª£c segment",
                        }
                    )
                    continue

                all_rows: List[Dict] = []
                for rj in results_list:
                    all_rows.extend(extract_word_rows(rj))

                rec_rows = [r for r in all_rows if (r.get("tok") or "").strip()]
                if not rec_rows:
                    table_rows.append(
                        {
                            "H·ªçc sinh": student,
                            "Audio QC": qc_msg,
                            "Score (%)": "",
                            "Missing words": "",
                            "WPM": "",
                            "Pause ratio": "",
                            "Long pauses": "",
                            "Sentence issues": "",
                            "Notable issues": "‚ö†Ô∏è Kh√¥ng c√≥ Words trong JSON",
                        }
                    )
                    continue

                score_pack = compute_word_based_score(
                    expected_tokens=expected_tokens,
                    rec_rows=rec_rows,
                    lookahead=int(lookahead_p),
                    missing_token_score=float(missing_token_score_p),
                )
                mapping = score_pack["mapping"]

                duration_s = float(metrics.get("duration_s", dur_s)) if metrics else float(dur_s)
                minutes = max(1e-6, duration_s / 60.0)
                wpm = len(rec_rows) / minutes
                pause_ratio = float(metrics.get("silence_ratio", 0.0)) if metrics else 0.0
                long_pauses = int(metrics.get("long_pause_count", 0)) if metrics else 0

                word_issues = build_passage_word_issues_concise(
                    expected_tokens=expected_tokens,
                    rec_rows=rec_rows,
                    mapping=mapping,
                    accuracy_threshold=float(accuracy_threshold_p),
                    s_acc_threshold=float(s_acc_threshold_p),
                    final_acc_threshold=float(final_acc_threshold_p),
                    ignore_stopwords=bool(ignore_stopwords),
                    lookahead=int(lookahead_p),
                    max_items=int(max_word_issues),
                )
                word_issues_by_student[student] = word_issues

                # Update global Top-issues aggregator (Tab 2)
                if word_issues:
                    _agg_add(st.session_state["top_issues_agg"], student, word_issues)

                sent_notes = detect_sentence_issues(
                    sentence_spans=sentence_spans,
                    expected_tokens=expected_tokens,
                    rec_rows=rec_rows,
                    mapping=mapping,
                    sentence_acc_threshold=float(sentence_acc_threshold),
                    pause_inside_sentence_ms=int(pause_inside_sentence_ms),
                    min_tokens_for_sentence_check=int(min_tokens_for_sentence_check),
                    max_notes=int(max_sentence_notes),
                )
                sentence_issues_by_student[student] = sent_notes

                table_rows.append(
                    {
                        "H·ªçc sinh": student,
                        "Audio QC": qc_msg,
                        "Score (%)": f"{score_pack['score_pct']:.1f}" if isinstance(score_pack.get("score_pct"), (int, float)) else "",
                        "Missing words": f"{score_pack['missing']}/{score_pack['total']}" if score_pack.get("total") else "",
                        "WPM": f"{wpm:.0f}",
                        "Pause ratio": f"{pause_ratio:.0%}",
                        "Long pauses": str(long_pauses),
                        "Sentence issues": " | ".join(sent_notes) if sent_notes else "",
                        "Notable issues": " | ".join(word_issues) if word_issues else "",
                    }
                )

        status.write("‚úÖ Xong!")
        progress.progress(100)

        csv_bytes = b""
        if pd is not None:
            df = pd.DataFrame(table_rows)
            st.dataframe(df, use_container_width=True)
            csv_bytes = df.to_csv(index=False).encode("utf-8-sig")
            st.download_button(
                "‚¨áÔ∏è T·∫£i CSV (Passage Level 1)",
                data=csv_bytes,
                file_name="passage_level1_summary.csv",
                mime="text/csv",
                use_container_width=True,
            )

        st.divider()
        st.subheader("üìå T√≥m t·∫Øt ƒë·ªÉ copy/paste g·ª≠i h·ªçc sinh")

        summary_lines = []
        for i, row in enumerate(table_rows, start=1):
            student = (row.get("H·ªçc sinh") or "").strip()
            qc = (row.get("Audio QC") or "").strip()

            if qc.startswith("‚ö†Ô∏è") or qc.startswith("‚ùå"):
                summary_lines.append(f"{i}. {student}: {qc}")
                continue

            score_txt = (row.get("Score (%)") or "").strip()
            miss_txt = (row.get("Missing words") or "").strip()
            wpm_txt = (row.get("WPM") or "").strip()
            pause_txt = (row.get("Pause ratio") or "").strip()

            sent_notes = sentence_issues_by_student.get(student, [])
            word_issues = word_issues_by_student.get(student, [])

            sent_text = (" | ".join(sent_notes)) if sent_notes else "(No sentence-level issue)"
            word_text = (" | ".join(word_issues)) if word_issues else "(No notable word issue)"

            summary_lines.append(
                f"{i}. {student} (Score {score_txt}%,)\n"
                f"   - Sentence: {sent_text}\n"
                f"   - Words: {word_text}"
            )

        full_text = "\n".join(summary_lines)

        # --------- SAVE SNAPSHOT (Tab 2) ----------
        try:
            os.makedirs(SESSION_DIR, exist_ok=True)
            if csv_bytes:
                save_bytes(os.path.join(SESSION_DIR, "passage_level1_summary.csv"), csv_bytes)
            save_text(os.path.join(SESSION_DIR, "passage_summary.txt"), full_text)

            append_log_json(SESSION_DIR, {
                "type": "passage",
                "class": chosen_class,
                "session": chosen_session,
                "locale": locale,
                "files_count": len(uploaded_files_passage) if uploaded_files_passage else 0,
                "saved_at": time.strftime("%Y-%m-%d %H:%M:%S"),
            })
        except Exception as e:
            st.warning(f"Snapshot save warning (Tab 2): {e}")

        st.text_area("Copy nhanh (Ctrl+A ‚Üí Ctrl+C)", value=full_text, height=320, key="t2_summary_text")
        st.markdown("#### Preview")
        for line in summary_lines:
            st.markdown(line.replace("\n", "  \n"))
        st.divider()
        st.subheader("üìñ Top 10 l·ªói ph·ªï bi·∫øn trong bu·ªïi h·ªçc n√†y ‚Äî Reading Passage")

        if st.session_state["top_issues_passage"]:
            _render_top10_issues(
                st.session_state["top_issues_passage"],
                title="",  # Kh√¥ng c·∫ßn title v√¨ ƒë√£ c√≥ subheader
                top_n=10
            )
        else:
            st.info("Ch∆∞a c√≥ d·ªØ li·ªáu l·ªói t·ª´ Tab Passage trong session n√†y.")
